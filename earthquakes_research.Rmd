---
title: "R Notebook"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

## Short description of our project

Team: Sofiia Popeniuk, Sofiia Sampara, Ostap Pavlyshyn

Link to dataset: <https://www.kaggle.com/datasets/patricklford/earthquakes-historical-data-and-live-data?select=earthquake_1995-2023.csv>

This dataset provides historical earthquake data from 1995 to 2022, capturing seismic events worldwide with details on location, magnitude, depth, time, etc. By examining this dataset alongside real-time seismic activity, we can better understand the relationship between plate tectonics, earthquakes, and geological hazards.

Key Variables:

```{r}
library(readr)
data <- read_csv("dataset/earthquake_1995-2023.csv")
head(data)
```

Short description what every variable mean: • Title is a name given to the earthquake for identification.

• Magnitude measures earthquake size, reflecting potential destruction.

• Date and Time records the timestamp of each earthquake.

• CDI is a maximum reported intensity of the earthquake as recorded by the community.

• MMI is a maximum estimated instrumental intensity, indicating the earthquake's observed effects.

• Alert is a classification of the event's alert level, with values such as “green,” “yellow,” “orange,” and “red,” indicating severity.

• Tsunami indicates whether the event occurred in an oceanic region ("1") or not ("0"), relevant for tsunami risk.

• Significance (sig) a numerical value describing the event's significance, factoring in magnitude, MMI, and reported impacts.

• Net is the data contributor's ID, signifying the network considered the preferred source of information.

• NST (Number of Seismic Stations) is a total number of seismic stations used to calculate the earthquake's location.

• Dmin is a horizontal distance from the epicenter to the nearest seismic station, influencing location accuracy.

• Gap is a largest azimuthal gap between adjacent stations, affecting the reliability of the calculated earthquake position.

• MagType is the algorithm or method used to determine the preferred magnitude.

• Depth below the Earth’s surface where the earthquake begins to rupture, influencing surface damage.

• Latitude/Longitude is a geographical coordinates for locating the earthquake on Earth's surface and analyzing regional trends.

• Location within the country affected by the earthquake.

• Continent here the affected country is located.

• Country that experienced the earthquake.

## The descriptive analysis of the dataset

1)  Reading the data from the file.

```{r}
head(data)
summary(data)
```

2)  Testing common distributions

```{r}
library(nortest)
analyze_variable <- function(var_name, data) {
  var_data <- data[[var_name]]
  var_data <- unique(var_data)
  if (is.character(var_data) || is.factor(var_data) || length(unique(var_data)) < 10) {
    cat("Categorical Variable:", var_name, "\n")
    counts <- table(var_data, useNA = "ifany")
    cat("\n")
  } else {
    cat("Numerical Variable:", var_name, "\n")
    var_data <- na.omit(as.numeric(var_data))
    if (length(var_data) > 1) {
      is_discrete <- all(var_data == floor(var_data))
      
      normal_test <- ad.test(var_data)$p.value
      exp_test <- ks.test(var_data, "pexp", rate = 1 / mean(var_data))$p.value
      unif_test <- ks.test(var_data, "punif", min = min(var_data), max = max(var_data))$p.value
      chi_square_test <- ks.test(var_data, "pchisq", df = mean(var_data))$p.value
      
      closest_dist <- ifelse(normal_test > 0.05, "Normal",
                             ifelse(exp_test > 0.05, "Exponential",
                                    ifelse(unif_test > 0.05, "Uniform",
                                                  ifelse(chi_square_test > 0.05, "Chi-Square", "Unknown"))))
      
      cat("  Is Discrete:", is_discrete, "\n")
      cat("  Normality test p-value:", normal_test, "\n")
      cat("  Exponential test p-value:", exp_test, "\n")
      cat("  Uniform test p-value:", unif_test, "\n")
      cat("  Chi-Square test p-value:", chi_square_test, "\n")
      cat("  Closest distribution:", closest_dist, "\n\n")
    } else {
      cat("  Insufficient data for analysis.\n\n")
    }
  }
}

for (var in colnames(data)) {
  analyze_variable(var, data)
}
```

```{r}
numeric_vars <- sapply(data, is.numeric)
numeric_data <- data[, numeric_vars]

num_vars <- sum(numeric_vars)
rows <- ceiling(sqrt(num_vars))
cols <- ceiling(num_vars / rows)

par(mfrow = c(rows, cols), mar = c(2, 2, 1, 0.5))

for (var_name in colnames(numeric_data)) {
  hist(numeric_data[[var_name]], 
       main = paste("Histogram of", var_name), 
       xlab = var_name, 
       col = "skyblue", 
       border = "white")
}

par(mfrow = c(1, 1))
```

3)  How many values of the data are NA?

```{r}
colSums(is.na(data))
```

A lot of data in the continent and country column are not defined (what to do with this? maybe, we should not take into account this data)

4)  Prepare data (make date-time column in proper format)

```{r}
library(lubridate)
data$date_time <- dmy_hm(data$date_time)
```

5)  Summary on some data

```{r}
library(ggplot2)
print("The summary of the magnitude of the earthquakes")
summary(data$magnitude)
print("The summary of the maximum reported intensity for the event")
summary(data$cdi)
print("The summary of the maximum estimated instrumental intensity for the event")
summary(data$mmi)
```

6)  Interesting distribution

```{r}
library(ggplot2)

cdi_mmi_diff <- data$cdi - data$mmi

ggplot(data.frame(cdi_mmi_diff), aes(x = cdi_mmi_diff)) +
  geom_histogram(binwidth = 0.3, fill = "blue", color = "black", alpha = 0.7) +
  labs(
    title = "Frequency Distribution of CDI - MMI Differences",
    x = "Difference (CDI - MMI)",
    y = "Frequency"
  ) +
  theme_minimal()

```

```{r}
table(data$alert)
table(data$magType)
```

```{r}
ggplot(data, aes(x = alert)) + geom_bar(fill = "green") +
    labs(title = "Alert Level Frequency", x = "Alert Level", y = "Count")
```

```{r}
ggplot(data, aes(x = date_time, y = sig)) + geom_line() +
    labs(title = "Earthquake Significance Over Time", x = "Date", y = "Significance")
```

# Hypothesis and Descriptions

## Hypothesis: Geographic Patterns of Magnitude

Hypothesis:\
Earthquakes in specific continents or regions (e.g., along tectonic plate boundaries) tend to have higher magnitudes than those in other regions.

Description:\
This hypothesis examines the geographic distribution of earthquake magnitudes, particularly focusing on whether regions associated with tectonic plate boundaries experience stronger earthquakes. By analyzing the "magnitude" variable in conjunction with "country\`, this hypothesis aims to identify spatial trends in seismic activity. This can provide insights into the global distribution of tectonic forces and their impact on earthquake magnitudes.

Statistical Hypotheses:\
- $H_0$: There is no association between earthquake magnitude category and continent. - $H_1$: There is an association between earthquake magnitude category and continent.

The provided R code output NA values in the dataset before and after data filling for specific columns. The code of preproccessing is in file fillCountry.py.

```{r}
library(knitr)
# Calculate NA counts for 'country' and 'magnitude' columns
na_country <- sum(is.na(data[['country']]))
na_magnitude <- sum(is.na(data[['magnitude']]))
na_continent <- sum(is.na(data[['continent']]))

na_summary <- data.frame(
  Column = c("country", "magnitude", "continent"),
  NA_Count = c(na_country, na_magnitude, na_continent)
)

# Display the summary table
kable(na_summary, caption = "Summary of missing values before filling data")


# Read the dataset with filled countries
data_country <- read_csv("dataset/earthquake_1995-2023-country.csv", show_col_types = FALSE)
na_country <- sum(is.na(data_country[['country']]))
na_magnitude <- sum(is.na(data_country[['magnitude']]))
na_continent <- sum(is.na(data_country[['continent']]))

na_summary_country <- data.frame(
  Column = c("country", "magnitude", "continent"),
  NA_Count = c(na_country, na_magnitude, na_continent)
)
kable(na_summary_country, caption = "Summary of missing values after filling data")

```

The code below creates a visual representation of earthquake locations on a world map.

```{r}
world_map <- map_data("world")
ggplot() + 
    geom_polygon(data = world_map, aes(x = long, y = lat, group = group), fill = "gray") +
    geom_point(data = data_country, aes(x = longitude, y = latitude, color = magnitude), alpha = 0.5) +
    labs(title = "Earthquake Locations", x = "Longitude", y = "Latitude")
```

```{r}
library(ggplot2)
#combining the same continents with different names
data_country$continent <- gsub("AF", "Africa", data_country$continent)
data_country$continent <- gsub("AS", "Asia", data_country$continent)
data_country$continent <- gsub("EU", "Europe", data_country$continent)
data_country$continent <- gsub("OC", "Oceania", data_country$continent)
data_country$continent <- gsub("SA", "South America", data_country$continent)
# Filter out rows with missing values
data_country <- data_country[!is.na(data_country$magnitude) & !is.na(data_country$continent), ]

# Plot all earthquake magnitudes by continent
ggplot(data_country, aes(x = continent, y = magnitude, color = continent)) +
  geom_jitter(alpha = 0.6, width = 0.2) + 
  labs(title = "Earthquake Magnitudes by Continent",
       x = "Continent",
       y = "Magnitude") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_color_brewer(palette = "Set1")

```

To apply a chi-squared Pearson test, both variables need to be categorical.The continent column is already categorized, but the magnitude column is continuous. We transform magnitude into a new variable called magnitude_category. The categorization logic involves magnitudes below 7.1 are categorized as "Low," those between 7.1 and 7.7 as "Medium," and those above 7.7 as "High." 

```{r}
#Fill magnitude_category
data_country$magnitude_category <- ifelse(
  is.na(data_country$magnitude), "NA",
  ifelse(data_country$magnitude < 7.1, "Low (<7.1)",
         ifelse(data_country$magnitude < 7.7, "Medium (<7.7)", "High (<8.3)"))
)

# The order of magnitude_category
data_country$magnitude_category <- factor(
  data_country$magnitude_category,
  levels = c("Low (<7.1)", "Medium (<7.7)", "High (<8.3)")
)

# Plot histogram
ggplot(data_country, aes(x = magnitude_category)) +
  geom_bar() +
  labs(title = "Distribution of Earthquake Magnitude Categories",
       x = "Magnitude Category",
       y = "Count") +
  theme_minimal()

```

The code creates a summary of the counts of earthquakes categorized by continent and magnitude_category. This table is allowing us to observe how the frequency of earthquake magnitudes is distributed across different continents.

```{r}
# Create a contingency table for the counts of earthquakes by category and country
contingency_table <- table(data_country$continent, data_country$magnitude_category)
print(contingency_table)

```

We grouped the continents with fewer counts into a category "Other" for better statistical analysis. This grouping helps to avoid problems with small cell sizes, which can make the test results incorrect.

To assess whether there is a statistically significant association between the grouped continent and earthquake magnitude category, we apply the chi-squared test with the "simulate.p.value = TRUE" argument. This approach is used when the expected values in some cells are too small for the approximation of the test to be correct.

```{r}
# Group continents with fewer counts into "Other"
data_country$continent_grouped <- ifelse(
  data_country$continent %in% c("Africa", "Europe", "Oceania"),
  "Other",
  data_country$continent
)

contingency_table_grouped <- table(data_country$continent_grouped, data_country$magnitude_category)
print(contingency_table_grouped)
#Perform test
chisq_test_sim <- chisq.test(contingency_table_grouped, simulate.p.value = TRUE)
print(chisq_test_sim)

```

Since the p-value is greater than the typical significance level of 0.05, we fail to reject the null hypothesis. There is not enough statistical evidence to say that magnitude (not) differs significantly across the continents.

# Using Heatmap

## Hypothesis: Proximity to Seismic Stations

Hypothesis:\
Earthquakes with smaller distances to the nearest seismic station (dmin) have more reliable intensity measurements, such as CDI (Community Internet Intensity Map) and MMI (Modified Mercalli Intensity).

Description:\
This hypothesis investigates whether the proximity of an earthquake to seismic stations influences the reliability of its intensity measurements. Earthquakes occurring closer to seismic stations are expected to provide more accurate and consistent data in terms of intensity (CDI and MMI) due to the higher density of monitoring equipment in the vicinity. By analyzing the correlation between dmin and intensity metrics, we aim to understand how distance from seismic stations may impact the quality of intensity reports.

Statistical Hypotheses:\
$H_0$: The mean distance to the nearest seismic station (dmin) does not differ significantly between earthquakes with reliable and less reliable intensity measurements (CDI, MMI).

$H_1$: The mean distance to the nearest seismic station (dmin) is smaller for earthquakes with reliable intensity measurements (CDI, MMI) than for those with less reliable intensity measurements.

```{r}
library(ggplot2)
library(reshape2)

cor_matrix <- cor(data[, c("dmin", "cdi", "mmi")])

# Melt the correlation matrix for ggplot
cor_melt <- melt(cor_matrix)

# Plot heatmap
ggplot(cor_melt, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile(color = "white") +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", midpoint = 0, 
                       limit = c(-1, 1), name = "Correlation") +
  theme_minimal() +
  labs(title = "Heatmap of Correlations", x = "", y = "")

# reliability based on cdi and mmi thresholds
data$reliability <- ifelse(data$cdi > 5 & data$mmi > 5, 1, 0)
table(data$reliability)

# Subset data
reliable <- subset(data, reliability == 1)
less_reliable <- subset(data, reliability == 0)

# Perform t-test on dmin
t_test_result <- t.test(reliable$dmin, less_reliable$dmin)
print(t_test_result)

```

The heatmap provides a visual representation of the correlations among "dmin`, "cdi`, and "mmi`. From the visualization, we observe a positive correlation between "cdi" and "mmi`, which is expected as both metrics measure earthquake intensity. However, the correlations between "dmin" and "cdi" is positive, and between "dmin" and "mmi" are negative.

The statistical hypothesis testing involved a t-test comparing the mean "dmin" values for earthquakes. The null hypothesis stated that the mean distance (`dmin`) does not significantly differ between the two groups, while the alternative hypothesis proposed that reliable earthquakes have smaller distances.

The t-test results had a p-value of 0.9545, above the typical significance threshold of 0.05. This high p-value indicates a failure to reject the null hypothesis.


## Hypothesis: Relationship Between Magnitude and Impact

Hypothesis:\
Earthquakes with higher magnitudes are associated with greater significance and higher reported intensities, such as CDI and MMI.

Description:\
This hypothesis explores whether larger earthquakes, as measured by magnitude, tend to result in more significant impacts and higher intensity values in reports such as CDI and MMI. Earthquakes with higher magnitudes are generally expected to cause more severe shaking, which could lead to higher reported intensity values. By analyzing the relationship between the magnitude variable and the impact measures (sig, CDI, MMI), this hypothesis aims to assess how magnitude correlates with the severity and significance of earthquake impacts.

Statistical Hypotheses:\
$H_0$: There is no significant correlation between earthquake magnitude and impact measures (sig, CDI, MMI).\
$H_1$: There is a significant positive correlation between earthquake magnitude and impact measures (sig, CDI, MMI).

Test for this hypothesis is described below.

------------------------------------------------------------------------

## Hypothesis: Depth and Intensity Relationship

Hypothesis:\
Shallower earthquakes (smaller depth) are associated with higher reported intensities (CDI) and instrumental intensities (MMI).

Description:\
This hypothesis examines whether the depth of an earthquake influences the intensity of its impact. Shallower earthquakes are generally expected to cause more noticeable ground shaking, leading to higher reported intensities, such as those found in the maximum reported intensity (CDI) and maximum estimated instrumental intensity (MMI). The hypothesis suggests that the depth of the earthquake has a negative relationship with intensity, meaning that as the depth decreases (the earthquake becomes shallower), the reported intensity (CDI and MMI) increases. By analyzing the relationship between earthquake depth and intensity, this hypothesis aims to understand how depth correlates with the perceived impact of an earthquake.

Statistical Hypotheses:\
$H_0$: There is no significant correlation between earthquake depth and intensity measures (CDI, MMI). $H_1$: There is a significant negative correlation between earthquake depth and intensity measures (CDI, MMI), with shallower earthquakes resulting in higher intensities.

Firstly, let's check which values are interesting to test. To do this we can create a correlation matrix and check which values are related and in what way. The values in matrix mean: 1 - a strong positive correlation -1 - a strong negative correlation.

```{r}
library(dplyr)
library(corrplot)
#Selecting columns we want to use
cor_data <- data %>% select(magnitude, depth, sig, cdi, mmi)

#Creating correlation matrix with Pearson  method
cor_matrix <- cor(cor_data, use = "complete.obs", method = "pearson")

print(cor_matrix)

#Plot the matrix
corrplot(cor_matrix, method = "color", type = "upper",
         title = "Correlation Matrix", tl.col = "black")
```

These are the rules when to use Pearson correlation:

(1) Both variables are continuous.
(2) The relationship between the variables is linear.
(3) Data is approximately normally distributed.

But some of our data violates those rules, so we can try another test - Spearman (For monotonic relationships, non-linear but consistent increase/decrease, the data contains outliers and this method is less sensitive to them)

The main idea of this test is similiar to the U-test, that we used previously.

The Spearman correlation coefficient, $r_s$, is calculated using the ranked values of the variables $X$ and $Y$: $r_s = 1 - \frac{6\sum d_i^2}{n(n^2-1)}$, where $d_i = Rank(X_i) - Rank(Y_i)$, $n$ is the number of observations.

```{r}
# Creating correlation matrix with Spearman method
cor_matrix_spearman <- cor(cor_data, use = "complete.obs", method = "spearman")

print(cor_matrix_spearman)

#Plot the correlation matrix
corrplot(cor_matrix_spearman, method = "color", type = "upper",
         title = "Correlation Matrix", tl.col = "black")
```

Here we can see that the relation between magnitude and significance is close to 1, meaning this two values have the positive correlation. Also, we can consider the correlation between depth and mmi, since this is the only negative correlation.

```{r}
# Taking data we want to test
x <- data$magnitude
y <- data$sig 

# Fit the data in the linear model
linear_model <- lm(y ~ x)

# Get some statistics about the model
summary(linear_model)

# Plot the data
plot(x, y, 
     pch = 16,
     main = "Linear Regression: Magnitude vs. Significance",
     xlab = "Magnitude",
     ylab = "Significance")

# Plot the regression line
abline(linear_model, col = "blue", lwd = 2)
```

Linear regression assumes a linear relationship between the independent variable x and the dependent variable y. In our case we have magnitude as x and significance as y. The relationship can be expressed as $y = \alpha + \beta x$, where $\alpha$ is an intercept and $\beta$ is a slope. We use a least squared method to estimate these parameters. 

The formula for sum of squared residuals:
$\sum (y_i - (\alpha + \beta x_i ))^2$

The regression process adjusts $\alpha$ and $\beta$ to minimize this SSR. The formulas for these estimates are:

$\beta = \frac{\sum(x_i−\bar{x})(y_i−\bar{y})}{\sum (x_i−\bar{x})^2}$

$\alpha = \bar{y}− \beta \bar{x}$

Conclusions:

The equation for linear regression becomes: $y = -1601.4 + 352.92x$ R-squared (which is the measure that indicates the proportion of the variance in the dependent variable y (significance) that is explained by the independent variable x (magnitude) in the model). It is calculated like this: $R^2 = 1 - \frac{SS_{residual}}{SS_{total}}$, where $SS_{residual} = \sum(y_i - \hat{y_i})^2$, $SS_{total} = sum(y_i - \bar{y_i})^2$. in our regression this statistic is much less than 0.7 indicating that our model does not provide a good fit.

```{r}
# Taking data we want to test
x <- data$depth
y <- data$mmi

# Fit the data in the linear model
linear_model <- lm(y ~ x)

# Get some statistics about the model
summary(linear_model)

# Plot the data
plot(x, y, 
     pch = 16,
     main = "Linear Regression: Depth vs. MMI",
     xlab = "Depth",
     ylab = "MMI")

# Plot the regression line
abline(linear_model, col = "blue", lwd = 2)
```

For the second hypothesis we also tried to fit the data in the linear regression, but we observed, that the fit is not good enough, so we tried to fit the data in the polynomial regression, since we see, that the plot suggest a bit curved line.

```{r}
# Get the data needed for test
x <- data$depth
y <- data$mmi

#Fitting the data in the polynomial model
poly_model <- lm(y ~ poly(x, 3))

# Get some statistics on the regression
summary(poly_model)

# Plot the data
plot(x, y, 
     pch = 16, col = "blue",
     main = "Polynomial Regression: Depth vs. MMI",
     xlab = "Depth", ylab = "MMI")

# Plot the regression
x_grid <- seq(min(x), max(x), length.out = 100) 
y_pred <- predict(poly_model, newdata = data.frame(x = x_grid))

lines(x_grid, y_pred, col = "red", lwd = 2)
```

This approach is almost the same as the linear regression, but uses linear algebra theory to find the formulas for estimates of parameters.

The form of this regression is:
$\hat{y_i} = \alpha + \beta_1x_i + \beta_2x_i^2 + \beta_3x_i^3$

To find the estimates for the parameters we also minimize the sum of squared residuals.

Conclusions: The polynomial regression $y = 6.27 - 23.43263x + 1.01327x^2 + 3.68435x^3$ does not fit into our data good enough, because the R-squared value is too small. We can observe, that our data does not form a good fit, which we can predict, but we clearly see some dependence of these two variables.

------------------------------------------------------------------------

## Hypothesis: Tsunami and Oceanic Earthquakes

Hypothesis:\
Earthquakes in oceanic regions (tsunami = 1) tend to have larger magnitudes and are more significant than non-oceanic earthquakes.

Description:\
This hypothesis investigates whether earthquakes that occur in oceanic regions and are associated with tsunamis tend to be more powerful and significant compared to earthquakes in non-oceanic regions. Tsunami-generating earthquakes, which are often located in oceanic regions, may be larger in magnitude and have greater significance due to the potential for widespread damage and coastal effects. By comparing earthquake magnitudes and significance values (sig) between oceanic earthquakes (tsunami = 1) and non-oceanic earthquakes (tsunami = 0), this hypothesis aims to determine if oceanic earthquakes are statistically more intense and impactful.

Statistical Hypotheses:\
$H_0$: The mean magnitude and significance of earthquakes in oceanic regions are equal to those in non-oceanic regions. $H_1$: The mean magnitude and significance of earthquakes in oceanic regions are greater than those in non-oceanic regions.

```{r}
#Read the data
data <- read.csv("dataset/earthquake_1995-2023.csv")

# Select the data with and without tsunami
oceanic <- data$magnitude[data$tsunami == 1]
non_oceanic <- data$magnitude[data$tsunami == 0]

# Testing the data on normality
print(shapiro.test(oceanic))
print(shapiro.test(non_oceanic))

```

Since our data does not follow a normal distribution, we try to transform the data (in different ways), but it did not change the distribution.

```{r}
#transforming the data
log_oceanic <- log(oceanic)
hist(log_oceanic, main = "Histogram of Transformed Data")

#Testing the data on normality
shapiro_result <- shapiro.test(log_oceanic)
print(shapiro_result)
```

After trying to make some transformation to make our distribution normal we decided to try another test:

Mann-Whitney U Test

Why do we choose this test?

1)  Does not require data to follow a normal distribution.

2)  Compares the median (not the mean) between the two groups.

Restrictions:

The two samples are independent. The measurement scale is ordinal, interval, or ratio. The shapes of the distributions in the two groups are assumed to be similar (but not identical).

Null Hypothesis ($H_0$): The distributions of the two groups are the same (oceanic and non-oceanic earthquakes). Alternative Hypothesis ($H_1$): One group tends to have larger values than the other (e.g., oceanic earthquakes have larger magnitudes)

The Mann-Whitney U test ranks all data points from both groups combined, then compares the ranks between groups. The U-statistic measures how much one group’s ranks differ from the other’s.

How to conduct this type of a test?

(1) We have two independent samples of different sizes ($n_1$ and $n_2$). Then we combine these two samples in one set of values of size $n = n_1 + n_2$.
(2) Next, we rank the combined data in ascending order, assigning the average rank to tied values. Then, we calculate $R_i$, which are the sum of ranks: $R_i = \text{Sum of ranks for Group i}$. In our case Group 1 is the oceanic earthquakes and 2 - non-oceanic.
(3) Next, we use these ranks to compute the test statistics ($U_1$ and $U_2$). For this we use formulas: $U_1 = R_1 - \frac{n_1(n_1 + 1))}{2}$ and $U_2 = R_2 - \frac{n_1(n_1 + 1))}{2}$ The test statistic $U = min(U_1, U_2)$.
(4) Since our samples have large sizes, we approximate the U-statistic by a normal distribution: $Z = \frac{U - \mu_U}{\sigma_U}$, where $\mu_U = \frac{n_1n_2}{2}$ and $\sigma_U = \sqrt{\frac{n_1n_2(n_1 + n_2 + 1)}{12}}$
(5) Now, we can find the p-value for this test: $p = P(Z \lt U)$, where $Z \sim N(0, 1)$. If our p-value is less then the significance level $\alpha$, then we reject the null hypothesis.

```{r}
#Test of the first hypothesis
test_result <- wilcox.test(oceanic, non_oceanic, alternative = "greater")

print(test_result)
```

Since p-value is greater than 0.05, we DO NOT reject $H_0$ (we can not reject, that the distribution of magnitude of oceanic and non-oceanic earthquakes does not differ).

```{r}
#Test of the second hypothesis
test_result <- wilcox.test(oceanic_sig, non_oceanic_sig, alternative = "greater")

print(test_result)
```

In significance data we see, that p-value is very close to 0.5, but still greater, so we CAN NOT reject the $H_0$ and can claim, that the distribution of significance in earthquakes with tsunami does not differ from the one without it. However, if we took the significance level of 0.1 than we would reject the null hypothesis and claim that the significance of an earthquake is greater on the earthquake with tsunami than without it.

Results: The magnitude of the earthquakes in two groups (the earthquakes with tsunami and without) has same distribution. However, the significance of the earthquake tends to be greater in oceanic earthquakes than non-oceanic ones.

------------------------------------------------------------------------

# Predicting Earthquake Alert Levels Using a Decision Tree Classifier

Objective:\
To predict the alert level of earthquakes (e.g., green, yellow, orange, red) based on earthquake features such as magnitude, depth, significance (sig), tsunami occurrence, and maximum instrumental intensity (mmi).

Description:\
We’re building a decision tree classifier to model the relationship between earthquake characteristics and their alert levels. An alert level is a categorical label that shows how serious and dangerous an earthquake is. We’ll use features like magnitude, depth, significance (sig), tsunami occurrence (binary: 1 if a tsunami is generated, 0 otherwise), and maximum instrumental intensity (mmi) to predict the alert level.\

-   The significance ($sig$) is calculated using columns like magnitude, cdi, tsunami, latitude/longitude, and location, combining earthquake size, reported intensity, proximity to populated areas, and tsunami generation to quantify overall impact.
-   The maximum instrumental intensity ($mmi$) is calculated using magnitude, depth, dmin, and nst, representing the strongest ground shaking recorded, adjusted for earthquake size, depth, and station coverage.

We’re choosing a decision tree classifier because it’s easy to understand and can handle complex relationships between features. The model will create a tree-like structure where conditions on the features split the data into smaller groups, and then it’ll assign an alert level based on these splits. This approach will give us clear insights into how different factors affect the alert level, and it’ll also make it easy to see how the model makes decisions.
